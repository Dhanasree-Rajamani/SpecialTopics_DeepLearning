{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanasree-Rajamani/SpecialTopics_DeepLearning/blob/main/Assignment%203/297_tensorflow_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Required Libraries"
      ],
      "metadata": {
        "id": "q_8EQR8y8JHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S2Pfej68yQ9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters\n",
        "\n",
        "Defining hyperparameters like batch size, block size, learning rate, etc., that will govern the training and model architecture."
      ],
      "metadata": {
        "id": "sdZ6f4F08Mlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "nmPiey8O7nM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading\n",
        "\n",
        "Loading text data, which is assumed to be a set of poems from the file 'poems_dataset.txt'.\n",
        "This is not shakespere dataset, this is another custom dataset which consists of different poems of various genre"
      ],
      "metadata": {
        "id": "x_ZJ3fze8Sqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/poems_dataset.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "MKTyEOoS7kct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Tokenization\n",
        "\n",
        "Creating a vocabulary by finding unique characters in the dataset. Mapping characters to unique integers for encoding and the reverse for decoding.\n",
        "\n",
        "Encoding the Data\n",
        "\n",
        "Converting the entire text into a sequence of integers."
      ],
      "metadata": {
        "id": "JOvufOK88mtJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwPrUs56-0fY"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encoding the data\n",
        "data = [stoi[c] for c in text]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Validation Split\n",
        "\n",
        "Splitting the data into a training set (90% of the data) and a validation set (the remaining 10%)."
      ],
      "metadata": {
        "id": "bLmzt-c_8vsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "\n",
        "train_data_tensor = tf.constant(data[:n], dtype=tf.int32)\n",
        "val_data_tensor = tf.constant(data[n:], dtype=tf.int32)"
      ],
      "metadata": {
        "id": "8LisTMDp8ywx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Data Generator\n",
        "\n",
        "Function get_batch randomly samples batches of data for training the model."
      ],
      "metadata": {
        "id": "A2hx25FC82iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data_tensor, batch_size, block_size):\n",
        "    start_indices = tf.random.uniform((batch_size,), 0, len(data_tensor) - block_size, dtype=tf.int64)\n",
        "    x_batch = tf.stack([data_tensor[start:start + block_size] for start in start_indices])\n",
        "    y_batch = tf.stack([data_tensor[start + 1:start + block_size + 1] for start in start_indices])\n",
        "    return x_batch, y_batch"
      ],
      "metadata": {
        "id": "hp1Plrb77htO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Transformer Components\n",
        "\n",
        "MultiHeadSelfAttention: This class defines the multi-head self-attention mechanism.\n",
        "\n",
        "TransformerBlock: This class represents a block in the Transformer architecture which includes the attention mechanism and a feed-forward neural network.\n",
        "\n",
        "Defining Transformer Components\n",
        "\n",
        "MultiHeadSelfAttention: This class defines the multi-head self-attention mechanism.\n",
        "\n",
        "TransformerBlock: This class represents a block in the Transformer architecture which includes the attention mechanism and a feed-forward neural network."
      ],
      "metadata": {
        "id": "fdV2OF_z86Fc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0QIKaWc3MXs"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        self.values = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.keys = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.queries = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.fc_out = layers.Dense(embed_size)\n",
        "\n",
        "    def call(self, values, keys, query):\n",
        "        N, seq_length, _ = query.shape\n",
        "        value_len, key_len = values.shape[1], keys.shape[1]\n",
        "\n",
        "        # Split embedding into self.head pieces\n",
        "        values = tf.reshape(values, (N, value_len, self.heads, self.head_dim))\n",
        "        keys = tf.reshape(keys, (N, key_len, self.heads, self.head_dim))\n",
        "        queries = tf.reshape(query, (N, seq_length, self.heads, self.head_dim))\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention = tf.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
        "        attention = attention / tf.math.sqrt(float(self.head_dim))\n",
        "        attention = tf.nn.softmax(attention, axis=-1)\n",
        "\n",
        "        out = tf.einsum(\"nhql,nlhd->nqhd\", attention, values)\n",
        "        out = tf.reshape(out, (N, seq_length, self.embed_size))\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_size, heads)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.feed_forward = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(forward_expansion * embed_size, activation=\"relu\"),\n",
        "                layers.Dense(embed_size),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, value, key, query):\n",
        "        attention = self.attention(value, key, query)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Main Model\n",
        "\n",
        "The BigramLanguageModel class represents the main model architecture comprising embedding layers, multiple Transformer blocks, and a final dense layer for predictions."
      ],
      "metadata": {
        "id": "Oq3UBvpo8-mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(keras.Model):\n",
        "    def __init__(self, vocab_size, embed_size, heads, n_layers, max_length, forward_expansion, dropout):\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.embedding = layers.Embedding(vocab_size, embed_size)\n",
        "        self.positional_embedding = layers.Embedding(max_length, embed_size)\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "            for _ in range(n_layers)\n",
        "        ]\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.fc_out = layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        N, seq_length = x.shape\n",
        "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
        "        out = self.embedding(x)\n",
        "        out += self.positional_embedding(positions)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            out = block(out, out, out)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "heO-pJIE7XjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Initialization and Training\n",
        "\n",
        "Instantiating the model and training it using the Adam optimizer and a sparse categorical cross-entropy loss. During training, the model's loss is printed at regular intervals."
      ],
      "metadata": {
        "id": "AXR90bQ59Dc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(\n",
        "    vocab_size,\n",
        "    n_embd,\n",
        "    n_head,\n",
        "    n_layer,\n",
        "    block_size,\n",
        "    forward_expansion=n_embd * 4,\n",
        "    dropout=dropout\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "generated_text = []\n",
        "for iteration in range(max_iters):\n",
        "    x_batch, y_batch = get_batch(train_data_tensor, batch_size, block_size)\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    if iteration % eval_interval == 0:\n",
        "        print(f\"Iteration {iteration}, Loss: {loss.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1Dfqd4i9C3c",
        "outputId": "811b8694-44ef-403e-9c9c-528c6f4f4b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f180f677eb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f180f677eb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 4.8210062980651855\n",
            "Iteration 100, Loss: 3.157886266708374\n",
            "Iteration 200, Loss: 3.16868257522583\n",
            "Iteration 300, Loss: 3.1558258533477783\n",
            "Iteration 400, Loss: 3.1271755695343018\n",
            "Iteration 500, Loss: 3.154634475708008\n",
            "Iteration 600, Loss: 2.640578508377075\n",
            "Iteration 700, Loss: 2.53776216506958\n",
            "Iteration 800, Loss: 2.5078542232513428\n",
            "Iteration 900, Loss: 2.3348848819732666\n",
            "Iteration 1000, Loss: 2.2853941917419434\n",
            "Iteration 1100, Loss: 2.428593158721924\n",
            "Iteration 1200, Loss: 2.347982406616211\n",
            "Iteration 1300, Loss: 2.3472790718078613\n",
            "Iteration 1400, Loss: 2.2813467979431152\n",
            "Iteration 1500, Loss: 2.3147552013397217\n",
            "Iteration 1600, Loss: 2.3841500282287598\n",
            "Iteration 1700, Loss: 2.22607421875\n",
            "Iteration 1800, Loss: 2.177729606628418\n",
            "Iteration 1900, Loss: 2.2903754711151123\n",
            "Iteration 2000, Loss: 2.1901493072509766\n",
            "Iteration 2100, Loss: 2.228200912475586\n",
            "Iteration 2200, Loss: 2.1776833534240723\n",
            "Iteration 2300, Loss: 2.1726367473602295\n",
            "Iteration 2400, Loss: 2.0799922943115234\n",
            "Iteration 2500, Loss: 2.109468460083008\n",
            "Iteration 2600, Loss: 2.132807731628418\n",
            "Iteration 2700, Loss: 2.0045294761657715\n",
            "Iteration 2800, Loss: 2.1032814979553223\n",
            "Iteration 2900, Loss: 2.095485210418701\n",
            "Iteration 3000, Loss: 2.147775650024414\n",
            "Iteration 3100, Loss: 2.0418128967285156\n",
            "Iteration 3200, Loss: 1.9891750812530518\n",
            "Iteration 3300, Loss: 1.9316742420196533\n",
            "Iteration 3400, Loss: 1.9867308139801025\n",
            "Iteration 3500, Loss: 2.044285297393799\n",
            "Iteration 3600, Loss: 1.8424088954925537\n",
            "Iteration 3700, Loss: 1.9134821891784668\n",
            "Iteration 3800, Loss: 1.8304897546768188\n",
            "Iteration 3900, Loss: 1.8188564777374268\n",
            "Iteration 4000, Loss: 1.9837206602096558\n",
            "Iteration 4100, Loss: 1.8125860691070557\n",
            "Iteration 4200, Loss: 1.773199439048767\n",
            "Iteration 4300, Loss: 1.8129127025604248\n",
            "Iteration 4400, Loss: 1.9080538749694824\n",
            "Iteration 4500, Loss: 1.9341700077056885\n",
            "Iteration 4600, Loss: 1.8424172401428223\n",
            "Iteration 4700, Loss: 1.7690889835357666\n",
            "Iteration 4800, Loss: 0.673249363899231\n",
            "Iteration 4900, Loss: 0.21466779708862305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Generation\n",
        "\n",
        "A function generate_text is provided to generate new text sequences using the trained model. Given a starting string, the model produces new text up to a specified length."
      ],
      "metadata": {
        "id": "VBqZerR59SR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, max_generate_length=2000):\n",
        "    # Convert start_string to tensor\n",
        "    input_eval = [stoi[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    generated_text = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(max_generate_length):\n",
        "        logits = model(input_eval)\n",
        "        # Use a multinomial distribution to predict the token returned by the model\n",
        "        predicted_id = tf.random.categorical(logits[:, 0, :], num_samples=1)[-1,0].numpy()\n",
        "\n",
        "\n",
        "        # Append the predicted token to the input string and the generated text\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        generated_text.append(itos[predicted_id])\n",
        "\n",
        "    return ''.join(generated_text)\n",
        "\n",
        "start_string = \" \"  # You can use a space, or any other starting token\n",
        "print(generate_text(model, start_string))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjT-NwcRht-N",
        "outputId": "7ec8b587-69b6-4dcc-b747-3d1afcc47091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "''aa\n",
            "\"aaa ------ aae âof'aaa--:-ae --aoeee-aea-----------aeeaaeai----- a:-a--âââo----eelaa-âââ'---------a---âaae---aaa----------leaeea a f'aeegâo---------aiaea:i----!a--l'i ------f--ae a---aaea---l-----\n",
            ".\"----Iâ '---aa-----leaaaeaaeaaeeaa -I----IâT--- - 'aaaeese-aa---ade '''aae---------------- --a aeai-- aa aaia--e-----ele------------- 'aeeaa---ae-f:--f''--aaaaaaa--SV---I--a--------aadaa--a---eaaaiaagââaeeeeaaaaaaae --'-------y-fe------- --\"--------------o---------------------'r--------:--\"-aa a ----aaia aeiaaaaeaiia-aaalaaeaaee-e--aia-aaei aaaa â-------------as-------------a\n",
            "âaaeeeee'eed'ae'''ae-----eee ee-----f-- ---f eeeaeae---e aaeei------aaaa aaaita--\"-aee,a aaeaae-------a -âoaeeaaae'---.SI''aas--------------ââade-âoee 'g'aeaaaa aaâaae--- -eaeaa---aee--a eeeeaaeegâââoaaeassa leeeaae -----I'----  ---ae\n",
            "''-a-I--aaae,----ââââ)aaeee aeeaaa eaa----------ad'eeeâaaaa aeaae-----a--y-Iaeaaee--aa-Oaaaa----aaaaaeeeai--------OI--------a:aaae-----\"---ee:'aaeaaegâ'' 'eaae ââleelei----LI--------------at\n",
            "ââo--aa '----ae ---aai -----------------aaaieeaeeaae---aae---fCFIsaleaaila-----\"eeaa-la--aeaaa------ae-------E--------a,raaaaa ae----a----aeaaee'--a-a-------------e-ae  aaae-----aaaaeeed:-SU-- -Iâo------i----ae-eaaa'aaee --\"I---I-------a--eaaea aaeaaa------aa--ae:------fi --------iie----,a----ae a-I--aaaaeeeeeasiaaaeeaeaae---r---I---aaaaia -aea aaay-----------f''aaeleaaa '''aeeaaa 'aaei--I-a------aaee-a eeaa----- ------aiaaa-------\"- asaa---M----eaa aeeaaaaeaea-asao'--eaa-------I''ee'âoe-!\"-'aeeaa--a-------- aaaealaeaa-''eaaa 'aai--------------\"---RUl---a âââaaee -a'aaaaaaa-a-aa eaea--S\n",
            "\"-aeeeae eaaaa --aaaeeassaialaaeaae--------aee -----âale------'a:'''ai ----l----- a--a -\"--ad:aa---aa----------------- 'aeee AI-------------I-aaaeaaa a------aeaeeaegaa---I--ale--------------iaaiae-aaaeeeea---a ---------eeeee-aea a-aeeaaeeea-aaan-f:----------aaa--aee'a-- aa aaai----a\n",
            "\"-aa-------aea--aeeeeeeaa--aaaaaae--a aea 'a-------I----------aeaeaeaaa--------e-----aae---'iaaa--'\n",
            "'-----I'ea,--\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxGd9alcLCmqId9VuaykjF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}