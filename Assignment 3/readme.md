Question 2: Transformers and finetuning with LLMs

a) Implement nanogpt from scratch using your own code in pytorch, tensorflow and jax

Text generation with NanoGPT using Pytorch: 

https://colab.research.google.com/drive/15KOzGX08XStRZyZIBwm1IBB6d08fgqKK?usp=sharing

Text generation with NanoGPT using Tensorflow: 

https://colab.research.google.com/drive/1eBa4mY_RJXG4XYd1zPErXg-Cn677K0Hq?usp=sharing

Text generation with NanoGPT using JAX: 

https://colab.research.google.com/drive/1krZKZE0dwNcCCWgoaB7X2Hug4ZNOFHMx?usp=sharing

![image](https://github.com/Dhanasree-Rajamani/SpecialTopics_DeepLearning/assets/111466424/0fb07463-4c58-4e57-938f-b4ef354fc374)

Medium Article: https://medium.com/@dhanasree.rajamani/text-generation-with-transformers-98fdd86f81aa

b) Implement "textbooks are all you need"

Use the existing model that is finetuned with python dataset, as well as generated a dataset with mental health related text, to finetune the model. This is used to build a chatbot which can guide user with their mental health. 

Colab: https://colab.research.google.com/drive/1rrhMVkVc_0TjeIUBMq7vVb-ONa1gHVI1?usp=sharing

![image](https://github.com/Dhanasree-Rajamani/SpecialTopics_DeepLearning/assets/111466424/03535d55-2d6d-4235-ad24-25cbe54b8f80)

![image](https://github.com/Dhanasree-Rajamani/SpecialTopics_DeepLearning/assets/111466424/dc02f5ba-521d-4996-ae7e-acf31c3ef651)

![image](https://github.com/Dhanasree-Rajamani/SpecialTopics_DeepLearning/assets/111466424/20c943b8-3b97-44a8-b866-24439f327716)

