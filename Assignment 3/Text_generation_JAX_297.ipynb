{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNXoBXTod06xazF6qb/u2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanasree-Rajamani/SpecialTopics_DeepLearning/blob/main/Assignment%203/Text_generation_JAX_297.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Required Libraries\n"
      ],
      "metadata": {
        "id": "ug6tz2kmu_D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit\n",
        "from flax import linen as nn\n",
        "import optax"
      ],
      "metadata": {
        "id": "xZiQ2r-WGGeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters\n",
        "\n",
        "Defining hyperparameters like batch size, block size, learning rate, etc., that will govern the training and model architecture."
      ],
      "metadata": {
        "id": "STB76kg8vCew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "learning_rate = 1e-3\n",
        "max_iters = 1000\n",
        "n_embd = 64\n",
        "vocab_size = 256  # Assuming ASCII\n",
        "\n",
        "rng_key = random.PRNGKey(0)"
      ],
      "metadata": {
        "id": "GnWiDPjnfwSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Tokenization\n",
        "\n",
        "Creating a vocabulary by finding unique characters in the dataset. Mapping characters to unique integers for encoding and the reverse for decoding.\n",
        "\n",
        "Encoding the Data\n",
        "\n",
        "Converting the entire text into a sequence of integers."
      ],
      "metadata": {
        "id": "psRtgtY9vKqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Data Generator\n",
        "\n",
        "Function get_batch randomly samples batches of data for training the model."
      ],
      "metadata": {
        "id": "hJi96IuzvOI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = x[..., None]  # Adding an embedding dimension\n",
        "        x = nn.Dense(n_embd)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.SelfAttention(num_heads=2)(x)\n",
        "        x = x.reshape((x.shape[0], x.shape[1], -1))  # Flattening the last dimensions\n",
        "        x = nn.Dense(vocab_size)(x)\n",
        "        return x\n",
        "\n",
        "@jit\n",
        "def softmax_cross_entropy(logits, targets):\n",
        "    logits_reshaped = logits.reshape((-1, vocab_size))\n",
        "    targets_reshaped = targets.reshape((-1,))\n",
        "    logprobs = jax.nn.log_softmax(logits_reshaped)\n",
        "\n",
        "    targets_one_hot = jax.nn.one_hot(targets_reshaped, vocab_size)\n",
        "\n",
        "    # Element-wise multiplication and sum over the vocab_size dimension\n",
        "    loss_values = -jnp.sum(targets_one_hot * logprobs, axis=-1)\n",
        "\n",
        "    # Reshape loss values back to (batch_size, block_size)\n",
        "    return loss_values.reshape((batch_size, block_size))\n",
        "\n",
        "@jit\n",
        "def compute_loss(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    loss_values = softmax_cross_entropy(logits, y)\n",
        "    mean_loss = jnp.mean(loss_values)\n",
        "    return mean_loss\n",
        "\n",
        "@jit\n",
        "def update(params, x, y, opt_state):\n",
        "    opt_update = optimizer.update\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, x, y)  # removed `model` from the arguments\n",
        "    updates, new_opt_state = opt_update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, loss\n",
        "\n",
        "\n",
        "# Data (for demonstration purposes, use real data in practice)\n",
        "data = jnp.array([i % vocab_size for i in range(10000)], dtype=jnp.int32)\n",
        "def get_batch():\n",
        "    idx = random.randint(rng_key, (batch_size,), 0, len(data) - block_size - 1)\n",
        "    x = jnp.array([data[i:i+block_size] for i in idx])\n",
        "    y = jnp.array([data[i+1:i+block_size+1] for i in idx])\n",
        "    return x, y\n",
        "\n",
        "# Training\n",
        "model = Transformer()\n",
        "params = model.init(rng_key, jnp.ones((batch_size, block_size)))\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    x, y = get_batch()\n",
        "    params, opt_state, loss = update(params, x, y, opt_state)\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss}\")\n",
        "\n",
        "# Additional utility function to convert a string to its ASCII representation\n",
        "def string_to_ascii(input_str):\n",
        "    return jnp.array([ord(c) for c in input_str], dtype=jnp.int32)\n",
        "\n",
        "# Simple text generation\n",
        "def generate_text(params, model, start_token=0, length=100):\n",
        "    generated = [start_token]\n",
        "\n",
        "    # Initialize a sequence of length `block_size` filled with the `start_token`\n",
        "    current_token = jnp.array([start_token] * block_size).reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)  # Generate logits for the sequence\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Append the next_token to current_token sequence and use only the last `block_size` tokens\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Initialize the model with a dummy input that matches the shape of our generation process\n",
        "dummy_input = jnp.ones((1, block_size))\n",
        "params_gen = model.init(rng_key, dummy_input)\n",
        "\n",
        "# Update the params with trained weights\n",
        "params_gen = params\n",
        "\n",
        "def generate_text(params, model, start_string, length=100):\n",
        "    start_tokens = string_to_ascii(start_string)\n",
        "    generated = list(start_tokens)\n",
        "\n",
        "    # If the initial tokens are fewer than block_size, pad them\n",
        "    if len(start_tokens) < block_size:\n",
        "        current_token = jnp.pad(start_tokens, (block_size - len(start_tokens), 0), mode='constant')\n",
        "    else:\n",
        "        current_token = start_tokens[-block_size:]  # Take the last `block_size` characters\n",
        "\n",
        "    current_token = current_token.reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Use the most recent `block_size` tokens for the next step\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return \"\".join([chr(c) for c in generated])\n",
        "\n",
        "print(generate_text(params_gen, model, start_string=\"once upon a time\", length=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngYi9lgIfqye",
        "outputId": "3b1e4349-fbad-4a63-d2ef-a133ecd9eaaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 6.21189546585083\n",
            "Iteration 100, Loss: 4.883039474487305\n",
            "Iteration 200, Loss: 4.875761985778809\n",
            "Iteration 300, Loss: 4.875408172607422\n",
            "Iteration 400, Loss: 4.875296592712402\n",
            "Iteration 500, Loss: 4.875243186950684\n",
            "Iteration 600, Loss: 4.875209808349609\n",
            "Iteration 700, Loss: 4.875184535980225\n",
            "Iteration 800, Loss: 4.875165939331055\n",
            "Iteration 900, Loss: 4.875140190124512\n",
            "once upon a timeõõõõõõõõõõõõõõõ\u0005>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "learning_rate = 1e-3\n",
        "max_iters = 4000\n",
        "n_embd = 64\n",
        "vocab_size = 256  # Assuming ASCII\n",
        "\n",
        "rng_key = random.PRNGKey(0)"
      ],
      "metadata": {
        "id": "_o8gszFWlJSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = x[..., None]  # Adding an embedding dimension\n",
        "        x = nn.Dense(n_embd)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.SelfAttention(num_heads=2)(x)\n",
        "        x = x.reshape((x.shape[0], x.shape[1], -1))  # Flattening the last dimensions\n",
        "        x = nn.Dense(vocab_size)(x)\n",
        "        return x\n",
        "\n",
        "@jit\n",
        "def softmax_cross_entropy(logits, targets):\n",
        "    logits_reshaped = logits.reshape((-1, vocab_size))\n",
        "    targets_reshaped = targets.reshape((-1,))\n",
        "    logprobs = jax.nn.log_softmax(logits_reshaped)\n",
        "\n",
        "    targets_one_hot = jax.nn.one_hot(targets_reshaped, vocab_size)\n",
        "\n",
        "    # Element-wise multiplication and sum over the vocab_size dimension\n",
        "    loss_values = -jnp.sum(targets_one_hot * logprobs, axis=-1)\n",
        "\n",
        "    # Reshape loss values back to (batch_size, block_size)\n",
        "    return loss_values.reshape((batch_size, block_size))\n",
        "\n",
        "@jit\n",
        "def compute_loss(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    loss_values = softmax_cross_entropy(logits, y)\n",
        "    mean_loss = jnp.mean(loss_values)\n",
        "    return mean_loss\n",
        "\n",
        "@jit\n",
        "def update(params, x, y, opt_state):\n",
        "    opt_update = optimizer.update\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, x, y)  # removed `model` from the arguments\n",
        "    updates, new_opt_state = opt_update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, loss\n",
        "\n",
        "\n",
        "# Data (for demonstration purposes, use real data in practice)\n",
        "data = jnp.array([i % vocab_size for i in range(10000)], dtype=jnp.int32)\n",
        "def get_batch():\n",
        "    idx = random.randint(rng_key, (batch_size,), 0, len(data) - block_size - 1)\n",
        "    x = jnp.array([data[i:i+block_size] for i in idx])\n",
        "    y = jnp.array([data[i+1:i+block_size+1] for i in idx])\n",
        "    return x, y\n",
        "\n",
        "# Training\n",
        "model = Transformer()\n",
        "params = model.init(rng_key, jnp.ones((batch_size, block_size)))\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    x, y = get_batch()\n",
        "    params, opt_state, loss = update(params, x, y, opt_state)\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss}\")\n",
        "\n",
        "# Additional utility function to convert a string to its ASCII representation\n",
        "def string_to_ascii(input_str):\n",
        "    return jnp.array([ord(c) for c in input_str], dtype=jnp.int32)\n",
        "\n",
        "# Simple text generation\n",
        "def generate_text(params, model, start_token=0, length=100):\n",
        "    generated = [start_token]\n",
        "\n",
        "    # Initialize a sequence of length `block_size` filled with the `start_token`\n",
        "    current_token = jnp.array([start_token] * block_size).reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)  # Generate logits for the sequence\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Append the next_token to current_token sequence and use only the last `block_size` tokens\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Initialize the model with a dummy input that matches the shape of our generation process\n",
        "dummy_input = jnp.ones((1, block_size))\n",
        "params_gen = model.init(rng_key, dummy_input)\n",
        "\n",
        "# Update the params with trained weights\n",
        "params_gen = params\n",
        "\n",
        "def generate_text(params, model, start_string, length=100):\n",
        "    start_tokens = string_to_ascii(start_string)\n",
        "    generated = list(start_tokens)\n",
        "\n",
        "    # If the initial tokens are fewer than block_size, pad them\n",
        "    if len(start_tokens) < block_size:\n",
        "        current_token = jnp.pad(start_tokens, (block_size - len(start_tokens), 0), mode='constant')\n",
        "    else:\n",
        "        current_token = start_tokens[-block_size:]  # Take the last `block_size` characters\n",
        "\n",
        "    current_token = current_token.reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Use the most recent `block_size` tokens for the next step\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return \"\".join([chr(c) for c in generated])\n",
        "\n",
        "print(generate_text(params_gen, model, start_string=\"\"\"The farmhouse lingers, though averse to square\n",
        "With the new city street it has to wear\n",
        "A number in. But what about the brook\n",
        "That held the house as in an elbow-crook?\n",
        "I ask as one who knew the brook, its strength\n",
        "And impulse, having dipped a finger length\n",
        "And made it leap my knuckle, having tossed\n",
        "A flower to try its currents where they crossed.\n",
        "The meadow grass could be cemented down\n",
        "From growing under pavements of a town;\n",
        "The apple trees be sent to hearth-stone flame.\n",
        "Is water wood to serve a brook the same?\n",
        "How else dispose of an immortal force\n",
        "No longer needed? Staunch it at its source\n",
        "With cinder loads dumped down? The brook was thrown\n",
        "Deep in a sewer dungeon under stone\n",
        "In fetid darkness still to live and run â€”\n",
        "And all for nothing it had ever done\n",
        "Except forget to go in fear perhaps.\n",
        "No one would know except for ancient maps\n",
        "That such a brook ran water. But I wonder\n",
        "If from its being kept forever under,\n",
        "The thoughts may not have risen that so keep\n",
        "This new-built city from both work and sleep.\"\"\", length=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru6ylrykk7FL",
        "outputId": "70f984db-3ff6-4e14-fb03-dcd67b1b470c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 6.21189546585083\n",
            "Iteration 100, Loss: 4.883039474487305\n",
            "Iteration 200, Loss: 4.875761985778809\n",
            "Iteration 300, Loss: 4.875408172607422\n",
            "Iteration 400, Loss: 4.875296592712402\n",
            "Iteration 500, Loss: 4.875243186950684\n",
            "Iteration 600, Loss: 4.875209808349609\n",
            "Iteration 700, Loss: 4.875184535980225\n",
            "Iteration 800, Loss: 4.875165939331055\n",
            "Iteration 900, Loss: 4.875140190124512\n",
            "Iteration 1000, Loss: 4.8751139640808105\n",
            "Iteration 1100, Loss: 4.87531042098999\n",
            "Iteration 1200, Loss: 4.875078201293945\n",
            "Iteration 1300, Loss: 4.875041961669922\n",
            "Iteration 1400, Loss: 4.875000953674316\n",
            "Iteration 1500, Loss: 4.874953746795654\n",
            "Iteration 1600, Loss: 4.874897480010986\n",
            "Iteration 1700, Loss: 4.874874114990234\n",
            "Iteration 1800, Loss: 4.874741554260254\n",
            "Iteration 1900, Loss: 4.874608993530273\n",
            "Iteration 2000, Loss: 4.874403476715088\n",
            "Iteration 2100, Loss: 4.874251365661621\n",
            "Iteration 2200, Loss: 4.873983383178711\n",
            "Iteration 2300, Loss: 4.873186111450195\n",
            "Iteration 2400, Loss: 4.879136085510254\n",
            "Iteration 2500, Loss: 4.8709611892700195\n",
            "Iteration 2600, Loss: 4.869430065155029\n",
            "Iteration 2700, Loss: 4.865708351135254\n",
            "Iteration 2800, Loss: 4.855800628662109\n",
            "Iteration 2900, Loss: 4.8351898193359375\n",
            "Iteration 3000, Loss: 4.717864990234375\n",
            "Iteration 3100, Loss: 4.695825099945068\n",
            "Iteration 3200, Loss: 4.684803009033203\n",
            "Iteration 3300, Loss: 4.525728225708008\n",
            "Iteration 3400, Loss: 4.438921928405762\n",
            "Iteration 3500, Loss: 4.358986854553223\n",
            "Iteration 3600, Loss: 4.363476276397705\n",
            "Iteration 3700, Loss: 4.367070198059082\n",
            "Iteration 3800, Loss: 4.236888408660889\n",
            "Iteration 3900, Loss: 4.207706928253174\n",
            "The farmhouse lingers, though averse to square\n",
            "With the new city street it has to wear\n",
            "A number in. But what about the brook\n",
            "That held the house as in an elbow-crook?\n",
            "I ask as one who knew the brook, its strength\n",
            "And impulse, having dipped a finger length\n",
            "And made it leap my knuckle, having tossed\n",
            "A flower to try its currents where they crossed.\n",
            "The meadow grass could be cemented down\n",
            "From growing under pavements of a town;\n",
            "The apple trees be sent to hearth-stone flame.\n",
            "Is water wood to serve a brook the same?\n",
            "How else dispose of an immortal force\n",
            "No longer needed? Staunch it at its source\n",
            "With cinder loads dumped down? The brook was thrown\n",
            "Deep in a sewer dungeon under stone\n",
            "In fetid darkness still to live and run â€”\n",
            "And all for nothing it had ever done\n",
            "Except forget to go in fear perhaps.\n",
            "No one would know except for ancient maps\n",
            "That such a brook ran water. But I wonder\n",
            "If from its being kept forever under,\n",
            "The thoughts may not have risen that so keep\n",
            "This new-built city from both work and sleep.EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "\n",
        "# Parameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "learning_rate = 1e-3\n",
        "max_iters = 1000\n",
        "n_embd = 64\n",
        "vocab_size = 256  # Assuming ASCII\n",
        "\n",
        "rng_key = random.PRNGKey(0)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = x[..., None]  # Adding an embedding dimension\n",
        "        x = nn.Dense(n_embd)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.SelfAttention(num_heads=2)(x)\n",
        "        x = x.reshape((x.shape[0], x.shape[1], -1))  # Flattening the last dimensions\n",
        "        x = nn.Dense(vocab_size)(x)\n",
        "        return x\n",
        "\n",
        "@jit\n",
        "def softmax_cross_entropy(logits, targets):\n",
        "    logits_reshaped = logits.reshape((-1, vocab_size))\n",
        "    targets_reshaped = targets.reshape((-1,))\n",
        "    logprobs = jax.nn.log_softmax(logits_reshaped)\n",
        "\n",
        "    targets_one_hot = jax.nn.one_hot(targets_reshaped, vocab_size)\n",
        "\n",
        "    # Element-wise multiplication and sum over the vocab_size dimension\n",
        "    loss_values = -jnp.sum(targets_one_hot * logprobs, axis=-1)\n",
        "\n",
        "    # Reshape loss values back to (batch_size, block_size)\n",
        "    return loss_values.reshape((batch_size, block_size))\n",
        "\n",
        "@jit\n",
        "def compute_loss(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    loss_values = softmax_cross_entropy(logits, y)\n",
        "    mean_loss = jnp.mean(loss_values)\n",
        "    return mean_loss\n",
        "\n",
        "@jit\n",
        "def update(params, x, y, opt_state):\n",
        "    opt_update = optimizer.update\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, x, y)  # removed `model` from the arguments\n",
        "    updates, new_opt_state = opt_update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, loss\n",
        "\n",
        "\n",
        "# Data (for demonstration purposes, use real data in practice)\n",
        "data = jnp.array([i % vocab_size for i in range(10000)], dtype=jnp.int32)\n",
        "def get_batch():\n",
        "    idx = random.randint(rng_key, (batch_size,), 0, len(data) - block_size - 1)\n",
        "    x = jnp.array([data[i:i+block_size] for i in idx])\n",
        "    y = jnp.array([data[i+1:i+block_size+1] for i in idx])\n",
        "    return x, y\n",
        "\n",
        "# Training\n",
        "model = Transformer()\n",
        "params = model.init(rng_key, jnp.ones((batch_size, block_size)))\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    x, y = get_batch()\n",
        "    params, opt_state, loss = update(params, x, y, opt_state)\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss}\")\n",
        "\n",
        "# Simple text generation\n",
        "def generate_text(params, model, start_token=0, length=100):\n",
        "    generated = [start_token]\n",
        "\n",
        "    # Initialize a sequence of length `block_size` filled with the `start_token`\n",
        "    current_token = jnp.array([start_token] * block_size).reshape(1, block_size)\n",
        "\n",
        "    for _ in range(length):\n",
        "        logits = model.apply(params, current_token)  # Generate logits for the sequence\n",
        "        next_token = jnp.argmax(logits[0, -1])\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Append the next_token to current_token sequence and use only the last `block_size` tokens\n",
        "        current_token = jnp.concatenate([current_token, next_token.reshape(1, 1)], axis=1)[:, -block_size:]\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "print(generate_text(params, model, start_token=0, length=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQnRoYz7E00e",
        "outputId": "5e215669-f035-47da-c08c-dab0c3344b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 6.21189546585083\n",
            "Iteration 100, Loss: 4.883039474487305\n",
            "Iteration 200, Loss: 4.875761985778809\n",
            "Iteration 300, Loss: 4.875408172607422\n",
            "Iteration 400, Loss: 4.875296592712402\n",
            "Iteration 500, Loss: 4.875243186950684\n",
            "Iteration 600, Loss: 4.875209808349609\n",
            "Iteration 700, Loss: 4.875184535980225\n",
            "Iteration 800, Loss: 4.875165939331055\n",
            "Iteration 900, Loss: 4.875140190124512\n",
            "[0, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 5, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhBUxfDePtpk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}