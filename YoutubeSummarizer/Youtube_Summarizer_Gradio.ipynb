{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wMO3w12o741",
        "outputId": "05e738ed-9fdc-4574-f7f3-a82f8c5fed24"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZOjczNejtfv",
        "outputId": "7093afb0-f03b-4fef-8962-b8d117d9659e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.111.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.11.17)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade google-api-python-client\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "b0b3CDIDjuIc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YOUTUBE_API_KEY = 'AIzaSyAWZtoP7bZxRWHUaxmzUtmY6csQ0m7KhVE'\n",
        "OPENAI_API_KEY = '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'"
      ],
      "metadata": {
        "id": "5q3JkAiblVjC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to extract youtube video ID from the URL"
      ],
      "metadata": {
        "id": "jXPBFhVBliF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_video_id_from_url(url):\n",
        "    \"\"\"\n",
        "    Extracts the YouTube video ID from a given URL.\n",
        "    \"\"\"\n",
        "    regex = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|\\S*?[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]{11})\"\n",
        "    match = re.search(regex, url)\n",
        "    return match.group(1) if match else None"
      ],
      "metadata": {
        "id": "NysK5T69ktJ4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to fetch the author, title and description of the youtube video"
      ],
      "metadata": {
        "id": "EKy_ZN40ltk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "def get_youtube_video_details(url, youtube_api_key):\n",
        "    \"\"\"\n",
        "    Fetches details of a YouTube video given its URL.\n",
        "    \"\"\"\n",
        "    video_id = extract_video_id_from_url(url)\n",
        "\n",
        "    # Initialize YouTube API client\n",
        "    youtube = build('youtube', 'v3', developerKey=youtube_api_key)\n",
        "    request = youtube.videos().list(part=\"snippet\", id=video_id)\n",
        "    response = request.execute()\n",
        "\n",
        "    if 'items' in response and len(response['items']) > 0:\n",
        "        title = response['items'][0]['snippet']['title']\n",
        "        author = response['items'][0]['snippet']['channelTitle']\n",
        "        description = response['items'][0]['snippet']['description']\n",
        "        return title, author, description\n",
        "    else:\n",
        "        return \"Video not found\", \"\", \"\"\n"
      ],
      "metadata": {
        "id": "J4eJ7qW_ky3L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound\n",
        "\n",
        "def get_video_transcript(video_url):\n",
        "    try:\n",
        "        video_id = extract_video_id_from_url(video_url)\n",
        "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "\n",
        "        # Fetch the transcript in English or the first available transcript\n",
        "        if transcript_list.find_transcript(['en']):\n",
        "            transcript = transcript_list.find_transcript(['en']).fetch()\n",
        "        else:\n",
        "            transcript = transcript_list[0].fetch()\n",
        "\n",
        "        # Combine the text of the transcript\n",
        "        combined_transcript = ' '.join([t['text'] for t in transcript])\n",
        "        return combined_transcript\n",
        "    except NoTranscriptFound:\n",
        "        return \"No transcript found for this video.\""
      ],
      "metadata": {
        "id": "4iO5zIsFpAvo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to summarize the youtube video using OpenAI API"
      ],
      "metadata": {
        "id": "wurDPGIkl2qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def summarize_text(text, openai_api_key):\n",
        "    \"\"\"\n",
        "    Summarizes the given text using OpenAI's GPT-3.\n",
        "    \"\"\"\n",
        "    openai.api_key = openai_api_key\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n\\n{text}\"}\n",
        "        ],\n",
        "        max_tokens=1500\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=4000):\n",
        "    \"\"\"\n",
        "    Splits text into smaller chunks, each with a maximum of chunk_size characters.\n",
        "    \"\"\"\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "def summarize_large_text(text, openai_api_key):\n",
        "    \"\"\"\n",
        "    Splits a large text into chunks and summarizes each chunk.\n",
        "    \"\"\"\n",
        "    chunks = split_text_into_chunks(text)\n",
        "    summaries = [summarize_text(chunk, openai_api_key) for chunk in chunks]\n",
        "    return ' '.join(summaries)\n",
        "\n"
      ],
      "metadata": {
        "id": "3YTvpB8ylzaG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example URL\n",
        "youtube_url = \"https://www.youtube.com/watch?v=l2BqNaAdMic&t=1s\" # Replace with any YouTube video URL\n",
        "\n",
        "# Fetch video details\n",
        "title, author, description = get_youtube_video_details(youtube_url, YOUTUBE_API_KEY)\n",
        "\n",
        "# Print video details\n",
        "print(f\"Title: {title}\\nAuthor: {author}\\nDescription: {description}\\n\")\n",
        "\n",
        "# Get summary of the description\n",
        "summary = summarize_text(description, OPENAI_API_KEY)\n",
        "print(f\"Summary: {summary}\")\n",
        "\n",
        "# Fetch the transcript\n",
        "transcript = get_video_transcript(youtube_url)\n",
        "\n",
        "# Check if a transcript was found\n",
        "if transcript != \"Unable to get the detailed summary for this video. Try another video!\":\n",
        "    # Get summary of the transcript\n",
        "    summary = summarize_text(transcript, OPENAI_API_KEY)\n",
        "    print(f\"Detailed Summary: {summary}\")\n",
        "else:\n",
        "    print(transcript)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srH1bl61l79s",
        "outputId": "2683f2d4-e759-4b3c-a308-b76712ce3466"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Generative AI based app using Vercel\n",
            "Author: Dhanasree Rajamani\n",
            "Description: In this video I will demonstrate how I developed and deployed a simple generative AI based web application using Vue, Vite and Vercel.\n",
            "\n",
            "Summary: The text explains that the author will showcase the development and deployment process of a basic generative AI web application using Vue, Vite, and Vercel.\n",
            "Detailed Summary: The text is a description of how to develop and deploy a simple generative AI-based web application using View.js and Versel. The author explains the process and provides steps on how to set up the project, make necessary changes to the code, create a GitHub repository, and deploy the application using Versel. They also demonstrate how to use the application to ask questions and get answers generated by Open AI. Overall, the text emphasizes the simplicity of creating and deploying the application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_url = \"https://www.youtube.com/watch?v=fHFOANOHwh8\"\n",
        "\n",
        "# Fetch video details\n",
        "title, author, description = get_youtube_video_details(youtube_url, YOUTUBE_API_KEY)\n",
        "\n",
        "# Print video details\n",
        "print(f\"Title: {title}\\nAuthor: {author}\\nDescription: {description}\\n\")\n",
        "\n",
        "# Get summary of the description\n",
        "summary = summarize_text(description, OPENAI_API_KEY)\n",
        "print(f\"Summary: {summary}\")\n",
        "\n",
        "# Fetch the transcript\n",
        "transcript = get_video_transcript(youtube_url)\n",
        "\n",
        "# Check if a transcript was found\n",
        "if transcript != \"Unable to get the detailed summary for this video. Try another video!\":\n",
        "    # Get summary of the transcript\n",
        "\n",
        "    # Use this function instead of summarize_text for large texts\n",
        "    large_text_summary = summarize_large_text(transcript, OPENAI_API_KEY)\n",
        "    print(f\"Detailed Summary: {large_text_summary}\")\n",
        "\n",
        "else:\n",
        "    print(transcript)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os1PcMc9mJzN",
        "outputId": "6f93df67-46ac-43ec-b470-7d10198263db"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Complete Exploratory Data Analysis And Feature Engineering In 3 Hours| Krish Naik\n",
            "Author: Krish Naik\n",
            "Description: Join the community session https://courses.ineuron.ai/Mega-Community-Live . Here All the materials will be uploaded.\n",
            "Download The Dataset: https://github.com/krishnaik06/5-Days-Live-EDA-and-Feature-Engineering\n",
            "TimeStamp:\n",
            "\n",
            "0:00:00 Introduction\n",
            "0:01:00 Zomato Dataset EDA\n",
            "0:59:25 Black Friday Sales EDA\n",
            "1:54:40 Flight Price Prediction EDA\n",
            "#KrishNaik #krishnaikhindi #eda #EDA #featurengineering\n",
            "Connect with me here:\n",
            "Twitter: https://twitter.com/Krishnaik06\n",
            "Facebook: https://www.facebook.com/krishnaik06\n",
            "instagram: https://www.instagram.com/krishnaik06\n",
            "\n",
            "Summary: This text is promoting a community session on a website called iNeuron.ai. The session will cover topics such as Zomato Dataset EDA, Black Friday Sales EDA, and Flight Price Prediction EDA. The session will also provide materials for download. The text also provides links to the dataset and the instructor's social media profiles.\n",
            "Detailed Summary: The text is a transcript of a video tutorial on exploratory data analysis (EDA) using a Zomato dataset. The speaker introduces the dataset and explains how to import necessary libraries and download the dataset. They also mention some errors encountered during importing the dataset and provide a solution. The text discusses importing a dataset and performing exploratory data analysis (EDA) using Pandas. It mentions the columns present in the dataset and their data types. The describe() function is used to obtain information about the numerical features, while df.info() provides information about all columns and their properties. The text also mentions the importance of identifying missing values and exploring both numerical and categorical variables. The instruction to find missing values in the dataset is given as df.isnull().sum(). The text discusses the presence of null values in a dataset. It mentions that the feature \"cosines\" has 9 missing values out of a total of 9551 rows. The author also demonstrates a code that identifies features with missing values and visualizes them using a heatmap. Additionally, there is a mention of another dataset called \"country code\" that the author attempts to read but encounters an error. The text discusses some problems with invalid characters and the need to use the read_excel function instead of read_csv for an Excel file. It also mentions merging two dataframes using the pd.merge function. The text then shows an example of combining data based on a common column. The resulting dataframe is saved as final_df. The text suggests exploring the data and mentions using the dtypes attribute to check the data types. The text also mentions discussing observations for certain operations. Finally, it mentions finding the count of different countries in the dataframe. The text discusses the distribution of Zomato records across different countries. It mentions that Zomato is mostly available in India and that the majority of transactions likely occur there. The text then explains how to retrieve and plot the number of records for each country using a pie chart. It suggests focusing on the top three countries that use Zomato, which are India, the United States, and the United Kingdom. Additionally, it mentions the possibility of adding percentage values to the chart. The text discusses the use of percentage formatting and observations from a pie chart. It then moves on to identifying numerical variables and exploring relationships using group by operations. The text mentions aggregating rating data and analyzing rating colors. The text discusses ratings and how they are categorized into different levels such as poor, good, very good, and excellent. The author shows how to convert the information into a data frame and rename columns. They mention that the data will be used to find correlations and plot visualizations. The text discusses the observation and analysis of ratings on the Zomato app. The writer notes that a significant number of ratings have a rating of zero, indicating that the app or the restaurant itself has not been rated. The writer then proceeds to analyze the ratings based on different ranges, such as 4.5-4.9 being excellent and 3.5-3.9 being good. They also mention that ratings between 2.5-2.9 are average and 2.0-2.4 are poor. The writer suggests drawing a bar plot to visualize the relationship between aggregate rating and rating count. The text discusses how to change the figure size in Matplotlib and how to color the heatmap based on ratings. It also mentions using different colors for the heatmap and provides code examples. The author suggests making observations from the colored heatmap. The text discusses observations about ratings, specifically the fact that the majority of ratings fall between 2.5 and 3.4. It then mentions the possibility of missing ratings and suggests using the average of ratings between 2.5 and 3.4 as a replacement. The text then moves on to plotting a count plot of rating colors using seaborn. It explains how to use count plot and provides an example of how the plot looks. The text then gives a task for the reader to find countries that have given zero ratings and mentions that this can be determined using either the aggregate rating or the rating color. The text discusses the analysis of a dataset related to customer ratings and online delivery options in different countries. The author explores the number of zero ratings and observes that a significant number of them are from Indian customers. They then proceed to determine which currency is used by each country and identify the countries that offer online delivery. The observations made include the availability of online delivery in India and the UAE. The text discusses creating a pie chart for city distribution using a data set. The author demonstrates the process of creating the pie chart by manipulating the data and plotting it using matplotlib. They encounter some errors in the code, which they troubleshoot. After successfully creating the pie chart, they analyze the data and find that New Delhi has the highest number of transactions. The text also mentions an assignment for the readers to find the top 10 food items and suggests visiting the author's website for more materials. The author concludes by mentioning upcoming sessions and thanking the readers. The text discusses the agenda for the day, which includes performing exploratory data analysis (EDA) and feature engineering on a Black Friday dataset. The goal is to clean and prepare the data for model training in order to predict the purchase amount of customers against various products. The dataset also includes customer demographics like age, gender, marital status, city type, stay in the current city, product details, and product category. The problem statement is to build a model that can predict the purchase amount and create personalized offers for customers. The text also mentions importing necessary libraries and reading the training and test datasets. The text discusses steps to combine a train and test data set for analysis. It explains the use of pandas functions such as merge and append to accomplish this. The importance of data preprocessing and understanding the types of features in the data is highlighted. The user id column is identified as redundant and suggested for deletion using the df.drop function. The text discusses data preprocessing in Python. It mentions dropping columns and converting categorical variables into numerical values. The author also provides code examples for converting the gender variable from \"female\" and \"male\" to 0s and 1s. The text discusses the process of handling categorical features, specifically age, in machine learning. The author explores different methods of encoding and converting the age feature into numerical values. They consider using dummy variables or ordinal encoding, and explain the importance of considering the distribution and frequency of age groups in the dataset. Ultimately, they suggest using ordinal encoding with specific numerical values assigned to each age group. The text discusses different techniques for label encoding in Python. It explains how to perform label encoding using a library and provides an example from a code snippet. The text also mentions using the pd.get_dummies function for encoding categorical variables and combining them with the original dataset using pd.concat. It emphasizes the importance of drop_first=True when encoding categories and mentions the need to repeat the steps for any new data. The text discusses various operations performed on a dataframe, including dropping a specific feature, checking for missing values, and replacing the missing values with the mode. The text also includes explanations of different types of features and suggestions for handling missing values in discrete features. In the given text, the author discusses how to handle missing values in certain categories of a dataset. They suggest using the mode to replace missing values in the \"product category 2\" and \"product category 3\" columns. They also suggest treating \"4 plus\" in the \"stay in current years\" column as simply \"4.\" The author demonstrates the code for replacing these missing values and explains the reasoning behind it. The text discusses checking and converting data types in a DataFrame. It mentions converting object type columns to integers and explains the process. It also mentions a visualization method called sns.pairplot but encounters an error. As an alternative, it suggests using a bar plot to compare age with purchase, specifically comparing the purchasing habits of men and women. It concludes by discussing the observation that the purchasing percentage of men is higher than women. The text discusses observations and visualizations of data related to purchases, occupations, and product categories. The author suggests that the data set is good for modeling and proposes using feature scaling using standard scalar. The text discusses the process of copying and pasting code from the internet and making adjustments. The writer encounters errors in their code and makes corrections. They mention the importance of consistent number of samples and dropping unnecessary columns. They share their code and plan to train their model using the provided dataset. The speaker is introducing a session on flight price prediction. They provide instructions on how to download the dataset and mention that the dataset includes date time information. They then proceed to import the dataset into pandas and display the first few rows of the training and test datasets. They mention that if the model score is not good with linear regression, other algorithms like decision tree regressor and random forest regressor can be tried. The text discusses the importance of common sense in job interviews and working in the real world industry. It mentions the use of different algorithms and hyperparameter tuning. The author combines train and test data sets into a final data frame and addresses the issue of NaN values in the prices. The text emphasizes the need for feature engineering due to the complex nature of the features in the data set. It suggests converting the date of journey field into a datetime format and deriving additional features such as day, month, and year. The author provides the code for splitting the date of journey using forward slashes to obtain the desired information. The text provides instructions on how to extract dates, months, and years from a given dataset. It suggests using the split function to separate the date into its individual components. The index 0 is used to obtain the date, the index 1 is used to obtain the month, and the index 2 is used to obtain the year. The code also includes a lambda function as an alternative method. After extracting the desired information, the code converts the date, month, and year into integers. Finally, it suggests dropping the \"date of journey\" feature from the dataset. The text discusses the process of feature engineering for a dataset, specifically focusing on the arrival time feature. It suggests splitting the arrival time to extract only the relevant information, such as the hour and minutes. The text also mentions the presence of null values in other features. The use of lambda functions and string splitting is recommended for performing these tasks. The text explains how to convert the arrival and departure times in a DataFrame from object type to integer type. It also discusses capturing the route information and the number of stops in the dataset. The author is discussing whether to delete a specific feature or not in the context of travel routes. They suggest focusing on the number of stops instead of other factors. They also mention using a unique function to identify total stops and replace any null values. They provide code examples for converting total stops into categorical values and dropping the route column. They then mention further feature engineering and encoding options for additional information. The text discusses attempting to convert a duration variable from hours and minutes into just minutes. The author explores using the split function to separate the hours and minutes, but encounters some difficulties with syntax. They eventually arrive at a solution by multiplying the hours by 60 and adding the minutes to get the total duration in minutes. They also mention that the variable is still in object format and attempt to convert it to the desired type. The text discusses a process of dropping records and performing label encoding for various features in a dataset called \"final_df\". It mentions using the drop() function to remove specific rows, and then using label encoding from the scikit-learn library to convert categorical features like airline, source, destination, and additional info into numerical values. The final dataframe has 14 columns and is shown as an example. The text discusses one-hot encoding, a technique used in machine learning to convert categorical data into a numerical format. The author mentions using the \"one hot encoder\" function from the scikit-learn library and the potential need to save the encoder as a pickle file for future use. The author encounters an error when executing the code and realizes that the input data should not be in series form. They make some adjustments and successfully perform one-hot encoding on the given data. The text also mentions an alternative method using the \"get dummies\" function from pandas to achieve the same result. The author suggests creating a new dataframe to hold the encoded data and provides an example code to demonstrate the process. The text is a short message expressing gratitude and well wishes for the day ahead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch the transcript\n",
        "def get_video_transcript(video_url):\n",
        "     video_id = extract_video_id_from_url(video_url)\n",
        "     try:\n",
        "         transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "         return ' '.join([t['text'] for t in transcript_list])\n",
        "     except:\n",
        "         return None\n",
        "\n",
        "# Function to segment the transcript\n",
        "def segment_transcript(transcript, max_words=500):\n",
        "     words = transcript.split()\n",
        "     segments = [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
        "     return segments\n",
        "\n",
        "# # Function to summarize text using OpenAI\n",
        "def summarize_text(text, openai_api_key):\n",
        "     openai.api_key = openai_api_key\n",
        "     response = openai.Completion.create(\n",
        "         engine=\"text-davinci-003\",\n",
        "         prompt=f\"Summarize the following text:\\n\\n{text}\",\n",
        "         max_tokens=100\n",
        "     )\n",
        "     return response.choices[0].text.strip()\n",
        "\n",
        "# Main function to generate the article\n",
        "def generate_article_from_transcript(video_url, openai_api_key):\n",
        "    transcript = get_video_transcript(video_url)\n",
        "    if not transcript:\n",
        "        return \"Transcript not available for this video.\"\n",
        "\n",
        "    segments = segment_transcript(transcript)\n",
        "    article = [\"Abstract: \" + summarize_large_text(transcript, openai_api_key)]\n",
        "\n",
        "    for segment in segments:\n",
        "        article.append(summarize_large_text(segment, openai_api_key))\n",
        "\n",
        "    article.append(\"Conclusion: \" + summarize_large_text(segments[-1], openai_api_key))\n",
        "    return \"\\n\\n\".join(article)\n",
        "\n",
        "\n",
        "video_url = \"https://www.youtube.com/watch?v=qzWZ4kO4dXI&t=2s\"\n",
        "article = generate_article_from_transcript(video_url, OPENAI_API_KEY)\n",
        "print(article)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R88kxRm7tm7C",
        "outputId": "4e2ff627-7198-43b1-f431-17b4352d6a82"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract: mark these are the results you see so if you read it here the orange lines which says Mr 7B is mistrals 7 billion um it has outperformed llama 7B by almost 6 percent okay both on the top 1 accuracy and it is also uh one more interesting thing I wanted to talk about was here so Mr 7B also has open source implementations like here you see it has pytorch implementation and that is a great thing okay and one more thing you see if you look at the tens ML 7B Instruct is an advanced language understanding model that is around 60 billion (compared to 7 billion from Lama and 13 billion from other models). It has a tremendous performance, outperforming 7 and 13 billion models, and is even close to the results of 34 billion models. It is better than 13 billion in the majority of benchmarks and human evaluation with the exception of MVPP benchmark. It can handle large sequences with lowering cost due to the sliding window attention methodology used. The tokenizer ating two columns in a single column\n",
            "\n",
            "This text explains how to install and use a powerful summarizing tool to summarize conversations. It describes how to install dependencies, load the training dataset into a single-column pandas data frame, and create a dialogue and summary column to summarize conversations. thousand steps will bring it to the next EPO this is all I wanted to explain you all.\n",
            "\n",
            "This text discusses the process of training a machine learning model using the mrl 7B instruct V 0.1 gptq tokenizer. The code is set to use catch Falls, replicate the pre-training performance, enable model gradient checkpointing, and prepare the model for int4 training. The training arguments are provided such as output directory, per device training batch size, gradient accumulation steps provided the raw formatmerge tokenizer and the model path and then perform inference\n",
            "\n",
            "This text details training and inference for a text model using the mRAL 7B function. The training arguments are set to use float16 and there is an option to push the adapter model to Hub. To run the inference, a model is provided, the text dataset field is set to 'text,' the PFconfig is provided, the training arguments is set to use a tokenizer and the maximum sequence length is This text is about the MISTL model, an open source beast that is the best 7B model right now. It explains how to set model parameters such as low CPU memory usage, device map, generation config, temperature, top K, Max new tokens, and pad token ID. The example provided in the text is a use case where the summary is \"He will meet Kik soon\" and the model has shown good performance. The author encourages readers to share the video with their friends This text encourages viewers to share the content with their friends and community, and to comment their opinions on the topic. It ends by saying goodbye until the next video.\n",
            "\n",
            "almost the final result of the model the performance of 7 billion model uh is almost out performing the 13 billion model and 34 billion model which is greatest and yeah guys it is powered here with group query attention\n",
            "\n",
            "Mr AI's latest model, MRL 7B, has been trending on the internet and has shown to outperform other 7B, 13B, and 34B models on many benchmarks. The model uses group query attention and sliding window attention to handle longer sequences for faster inferencing.\n",
            "\n",
            "so\n",
            "\n",
            "X-Win is an open-source model with an Apache 2.0 license that outperforms other models, such as GPT 4 and LLAMA, when fine-tuning on various tasks. It has a higher performance than LLAMA in the MLU benchmark, and has similar performance to LLAMA 34B in the Knowledge and Reasoning benchmark. Its performance is around 60 billion in the MLU benchmark, and approximately 45 in the knowledge and reasoning benchmark.\n",
            "\n",
            "this is the model peep summary: The Hugging Face Temps has developed a model called 7B instruct which has outdone various benchmarks and can now be used to run models with 7 billion parameters. It utilizes a sliding window attention which allows it to handle 128k tokens, as well as a BPE tokenizer which ensures no token is left untranslated.\n",
            "\n",
            "the PFT adapter we need PFT bits and bytes we'll install them also Transformers is for the hug everybody you know the hugging face library itself in which we can access the different transformers models which we are using so this is what the installation needs\n",
            "\n",
            "In this video, we are going to fine tune a model called ML 7B Intrct V 0.1 which is 15 gigs in size. We will use GPT-Q for fine tuning the model as that is the only available\n",
            "\n",
            "hugging face models library okay\n",
            "\n",
            "Laura is installing Transformers from Source TRL, Pi 7ZR, and GPT-2 for quantization related things. She is also installing Optimum for fine-tuning GPT-2 models. They are using a data set called Samsung, but it is not commercially licensed. She is combining the dialogue and the summarization in order to have a prefix stating the dialogue and the assistant's summary. The tokenizer they are using is MRL 7B\n",
            "\n",
            "models\n",
            "\n",
            "In this text, we are exploring how to set up a model for training, which requires setting use catch Falls, training preprocessing, model gradient checkpointing, and calling the get PF model function for the model and configuration. This is important to prevent a code memory error and will hopefully replicate pre-training performance. Additionally, the model will map to a single device due to being a single GPU. Finally, the model printed is almost similar to Llamas and the keys are the\n",
            "\n",
            "linear which is not included in your training argument so you can follow this guy and yeah\n",
            "\n",
            "This text explains the arguments used in a code to perform a quantum model training to optimize a model for inference. It includes specifying the output directory, bat size, gradient accumulation steps, Optimizer, learning rate, and scheduler, among other things. Additionally, it mentions a specific function called mral 7B for training, a linear function called Quant linear, and pushing an Adapter model to the Hub.\n",
            "\n",
            "it's going to be very slow here like so we are downloading and essentially it's doing something here okay so yeah and once this is done we should get the result.\n",
            "\n",
            "Linear ok, they have a separate class called \"Mral Attention\" and another class called \"Quant Linear\". They have used a 'silly' activation function. The inner dimension they have worked on is 4096 and the loss they have achieved is 1.77 on 2,000 samples. They are saving the adapter\n",
            "\n",
            "printing the running time also.\n",
            "\n",
            "The model is set up to use low CPU memory usage and a float 16 data type with a device map as Cuda or a dict with empty string as key and value of zero. The generate function is separate and has various parameters such as temperature, Max new tokens (set to 25) and pad token ID to EOS token ID. The time() function is used to note start time and the model's generate function is given start star inputs. The output\n",
            "\n",
            "V is at the railway station and everything is going smoothly. He will soon meet Kik. Mistell is an open source beast and the best 7B model right now. V is currently working on an interesting use case with Mistell and plans to make an announcement soon. Mistell has shown great performance and can be tuned for a specific use case. Share this knowledge with friends and comment your opinions in the comments.\n",
            "\n",
            "Conclusion: V is at the railway station and everything is going smoothly. Mistell is an open-source Beast and is the best 7B right now. V is working on an interesting use case with Mistell which has shown some potential. Mistell can be tuned to harness its potential. Everyone should share this model with their friends and community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio"
      ],
      "metadata": {
        "id": "6ljRRBbTdQFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O42iC2R0dTxS",
        "outputId": "420c6795-1d45-4092-e4d6-ea9134725ab7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.9.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.105.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.7.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.3)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.3->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.3->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "YOUTUBE_API_KEY = 'AIzaSyAWZtoP7bZxRWHUaxmzUtmY6csQ0m7KhVE'\n",
        "OPENAI_API_KEY = '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'"
      ],
      "metadata": {
        "id": "ixWTbKnYjIaS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define the functions you've written here, such as extract_video_id_from_url, get_youtube_video_details, get_video_transcript, summarize_text, and summarize_large_text\n",
        "\n",
        "\n",
        "def process_youtube_video(url):\n",
        "    # Fetch video details\n",
        "    title, author, description = get_youtube_video_details(url, YOUTUBE_API_KEY)\n",
        "\n",
        "    # Get summary of the description\n",
        "    description_summary = summarize_text(description, OPENAI_API_KEY)\n",
        "\n",
        "    # Fetch the transcript\n",
        "    transcript = get_video_transcript(url)\n",
        "    if transcript:\n",
        "        transcript_summary = summarize_large_text(transcript, OPENAI_API_KEY)\n",
        "    else:\n",
        "        transcript_summary = \"Transcript not available.\"\n",
        "\n",
        "    return title, author, description_summary, transcript_summary\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_youtube_video,\n",
        "    inputs=gr.Textbox(label=\"YouTube Video URL\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Video Title\"),\n",
        "        gr.Textbox(label=\"Video Author\"),\n",
        "        gr.Textbox(label=\"Description Summary\"),\n",
        "        gr.Textbox(label=\"Transcript Summary\")\n",
        "    ],\n",
        "    live=False\n",
        ")\n",
        "\n",
        "iface.launch(debug = True)\n"
      ],
      "metadata": {
        "id": "bllQ2ZX9F8Tn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "outputId": "f98299ba-f730-4b0a-d39f-8ce55f861dfe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://feb800f4788d4a508c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://feb800f4788d4a508c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://feb800f4788d4a508c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3SbouqUnnXA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}