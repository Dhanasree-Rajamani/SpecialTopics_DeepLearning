{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQox9Kitf0P3wb0RrABEcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanasree-Rajamani/SpecialTopics_DeepLearning/blob/main/Assignment%202/Prompt_Refiner_application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqVUbacSW1A8",
        "outputId": "f868f196-098b-4b1b-95b5-c92a7aa9d9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-3.44.4-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.5.1 (from gradio)\n",
            "  Downloading gradio_client-0.5.1-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.12)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=e1cf711a4036fe563e938a42f52ff79191fa47e8e3ec175bf67ab5832155e321\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, openai, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.103.1 ffmpy-0.3.1 gradio-3.44.4 gradio-client-0.5.1 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.17.3 openai-0.28.1 orjson-3.9.7 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uvicorn-0.23.2 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "\n",
        "# # Initialize OpenAI API (make sure you have the necessary credentials)\n",
        "# openai.api_key = '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'\n",
        "\n",
        "# def critique_prompt(prompt_text):\n",
        "#     response = openai.Completion.create(\n",
        "#         engine=\"text-davinci-003\",\n",
        "#         prompt=f\"As an expert prompt engineer, ask questions on the user requirements and provide an improved well defined prompt: '{prompt_text}'. Give detailed steps on your thought process. The prompt should follow best practices in prompt engineering. List out what best practices you followed before giving your updated prompt\",\n",
        "#         max_tokens=1000\n",
        "#     )\n",
        "#     return response.choices[0].text.strip()\n"
      ],
      "metadata": {
        "id": "g07EtrwzW7Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "\n",
        "# def app_interface(prompt_text):\n",
        "#     critique = critique_prompt(prompt_text)\n",
        "#     return critique\n",
        "\n",
        "# interface = gr.Interface(\n",
        "#     fn=app_interface,\n",
        "#     inputs=gr.inputs.Textbox(label=\"Enter Your Prompt\"),\n",
        "#     outputs=gr.outputs.Textbox(label=\"Refined Prompt\"),\n",
        "#     live=False\n",
        "# )\n",
        "\n",
        "# interface.launch()\n"
      ],
      "metadata": {
        "id": "83jQn1sOXf9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "# import gradio as gr\n",
        "\n",
        "# # Initialize OpenAI API (make sure you have the necessary credentials)\n",
        "# openai.api_key = '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'\n",
        "\n",
        "# def critique_prompt(prompt_text):\n",
        "#     response = openai.Completion.create(\n",
        "#         engine=\"text-davinci-003\",\n",
        "#         prompt=f\"As an expert prompt engineer, ask questions on the user requirements and provide suggestions for improving: '{prompt_text}'. The final prompt given should be well-defined, follow best practices in prompt engineering. Please give details step by step on how you've generated the prompt and what best practices it follows\",\n",
        "#         max_tokens=500\n",
        "#     )\n",
        "#     # We expect a series of questions to be returned.\n",
        "#     questions = response.choices[0].text.strip().split('\\n')\n",
        "#     return questions"
      ],
      "metadata": {
        "id": "8a3UQdJMX8oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def app_interface(prompt_text):\n",
        "#     questions = critique_prompt(prompt_text)\n",
        "#     user_answers = []\n",
        "\n",
        "#     # For each question, get user feedback and refine the prompt.\n",
        "#     for q in questions:\n",
        "#         answer = gr.Interface(\n",
        "#             fn=lambda x: x,  # This is a dummy function to just collect and return the user's answer.\n",
        "#             inputs=gr.inputs.Textbox(lines=5, label=q),\n",
        "#             outputs=gr.outputs.Textbox(),\n",
        "#             live=False\n",
        "#         ).launch()\n",
        "#         user_answers.append(answer)\n",
        "\n",
        "#     # Check if user is ready to generate the final prompt.\n",
        "#     confirmation = gr.Interface(\n",
        "#         fn=lambda x: x,\n",
        "#         inputs=gr.inputs.Textbox(label=\"Have you given all the necessary details, are we ready to generate the prompt?\"),\n",
        "#         outputs=gr.outputs.Textbox(),\n",
        "#         live=False\n",
        "#     ).launch()\n",
        "\n",
        "#     if \"yes\" in confirmation.lower():\n",
        "#         # Use the user's answers to refine the prompt and generate the final version.\n",
        "#         # Here, we're making a simple concatenation for the example. But in a real-world application,\n",
        "#         # the way you refine the prompt will be more sophisticated.\n",
        "#         refined_prompt = prompt_text + ' ' + ' '.join(user_answers)\n",
        "#         return refined_prompt\n",
        "#     else:\n",
        "#         return \"Please provide more details.\""
      ],
      "metadata": {
        "id": "KHYELuVqZBi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interface = gr.Interface(\n",
        "#     fn=app_interface,\n",
        "#     inputs=gr.inputs.Textbox(label=\"Enter Your Prompt\"),\n",
        "#     outputs=gr.outputs.Textbox(label=\"Refined Prompt\"),\n",
        "#     live=False\n",
        "# )\n",
        "\n",
        "# interface.launch()\n"
      ],
      "metadata": {
        "id": "5tdYcMJXY90u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "\n",
        "# Initialize OpenAI API (make sure you have the necessary credentials)\n",
        "openai.api_key = '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'\n",
        "\n",
        "step = 0\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "def critique_prompt(prompt_text):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"As an expert prompt engineer, ask questions on the user requirements and provide suggestions for improving the prompt: '{prompt_text}'.\",\n",
        "        max_tokens=500\n",
        "    )\n",
        "    # Extract the series of questions (limited to 4 questions max).\n",
        "    qs = response.choices[0].text.strip().split('\\n')[:4]\n",
        "    return qs\n",
        "\n",
        "def generate_final_prompt():\n",
        "    combined_text = ' '.join(answers)\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"Do not respond to user question/requirement. An important step is to describe the step-by-step process on how the final prompt has been designed, mention the best practices followed in this prompt generation. The final prompt should be effective way to mention user's requirements to chatGPT. Generate the refined prompt, display the final refined prompt based on: '{combined_text}'.\",\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "def app_interface(prompt_text):\n",
        "    global step, questions, answers\n",
        "\n",
        "    # Initial step: get the questions from the model.\n",
        "    if step == 0:\n",
        "        questions = critique_prompt(prompt_text)\n",
        "        step += 1\n",
        "        return questions[step-1]\n",
        "\n",
        "    # Intermediate steps: collect user answers and ask subsequent questions.\n",
        "    elif 0 < step < len(questions):\n",
        "        answers.append(prompt_text)\n",
        "        question_to_ask = questions[step]\n",
        "        step += 1\n",
        "        return question_to_ask\n",
        "\n",
        "    # Final step: check if user is ready to generate the final prompt.\n",
        "    elif step == len(questions):\n",
        "        answers.append(prompt_text)\n",
        "        step += 1\n",
        "        return \"Have you given all the necessary details, are we ready to generate the prompt?\"\n",
        "\n",
        "    # Generate refined prompt with explanations.\n",
        "    else:\n",
        "        if \"yes\" in prompt_text.lower():\n",
        "            final_output = generate_final_prompt()\n",
        "            # Reset step for future interactions.\n",
        "            step = 0\n",
        "            return final_output\n",
        "        else:\n",
        "            step = 0\n",
        "            return \"Please provide more details.\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=app_interface,\n",
        "    inputs=gr.inputs.Textbox(label=\"Enter Your Prompt or Answer to the Question\"),\n",
        "    outputs=gr.outputs.Textbox(label=\"Refined Prompt or Explanation\"),\n",
        "    live=False\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "YfxFmAmBZC7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "456c6a8a-0edc-4a3e-becb-95d1333b7861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-eac1124a422a>:65: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  inputs=gr.inputs.Textbox(label=\"Enter Your Prompt or Answer to the Question\"),\n",
            "<ipython-input-11-eac1124a422a>:65: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  inputs=gr.inputs.Textbox(label=\"Enter Your Prompt or Answer to the Question\"),\n",
            "<ipython-input-11-eac1124a422a>:65: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  inputs=gr.inputs.Textbox(label=\"Enter Your Prompt or Answer to the Question\"),\n",
            "<ipython-input-11-eac1124a422a>:66: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  outputs=gr.outputs.Textbox(label=\"Refined Prompt or Explanation\"),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7864, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gz5VvVfjZ2Rs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}